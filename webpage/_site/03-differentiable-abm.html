<!DOCTYPE html>
<html lang="en"><head>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title> </title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta name="author" content="Arnau Quera-Bofarull" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A (nearly) no-CSS, fast, minimalist Jekyll theme." />
<meta property="og:description" content="A (nearly) no-CSS, fast, minimalist Jekyll theme." />
<link rel="canonical" href="https://riggraz.dev/https://largeagentcollider.github.io/icaif_tutorial/03-differentiable-abm.html" />
<meta property="og:url" content="https://riggraz.dev/https://largeagentcollider.github.io/icaif_tutorial/03-differentiable-abm.html" />
<meta name="twitter:card" content="summary" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Arnau Quera-Bofarull"},"@type":"WebPage","description":"A (nearly) no-CSS, fast, minimalist Jekyll theme.","url":"https://riggraz.dev/https://largeagentcollider.github.io/icaif_tutorial/03-differentiable-abm.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://riggraz.dev/https://largeagentcollider.github.io/icaif_tutorial/feed.xml" title=" " /><link rel="shortcut icon" type="image/x-icon" href="/https://largeagentcollider.github.io/icaif_tutorial/logo.png" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <header>
  <h1> </h1></header><ul></ul><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<h1 id="differentiable-agent-based-models">Differentiable Agent-Based Models</h1>

<p>We are now in a good spot to code our first differentiable ABM. Refer to notebooks 1 and 2 for a review of automatic differentiation (AD).</p>

<h1 id="1-random-walk">1. Random Walk</h1>

<p>Let us first consider a very simple ABM: a 1-dimensional random walk. The model is defined through the recursion</p>

\[\begin{align}
\xi &amp;\sim \mathrm{Bernoulli}({\theta}),\\
x_{t+1} &amp;= x_t + \begin{cases}1 &amp;\mathrm{if} &amp;\xi = 1 \\ -1 &amp;\mathrm{if} &amp;\xi = 0\end{cases}
\end{align}\]

<p>A first naive implementation using PyTorch might be:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">).</span><span class="nf">sample</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">xi</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x13eb442e0&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_3_1.png" alt="png" /></p>

<p>Let us now compute the Jacobian</p>

\[(J)_i = \frac{\partial {x_i}}{\partial \theta}\]

<p>which we will later use for calibration. If we try to use torch’s autograd to compute the jacobian:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x13ec03790&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_5_1.png" alt="png" /></p>

<p>We obtain a gradient of 0. Why is that? There are two reasons:</p>

<ol>
  <li>As we noted in the previous tutorials, the Bernoulli distribution is not automatically differentiable, and we need to use a continuous relaxation such as Gumbel-Softmax.</li>
  <li>AD frameworks such as PyTorch require a static computation graph to perform AD. Even though they support control flow statement such as <code class="language-plaintext highlighter-rouge">if</code> or <code class="language-plaintext highlighter-rouge">else</code>, they do not support control flow statements that depend on the parameters that we want to differentiate to. This can be circumvented by using masks. That is, a statement</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="k">if</span> <span class="n">x</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">a</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">b</span>
</code></pre></div></div>

<p>can be written as</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributions</span><span class="p">.</span><span class="nc">Bernoulli</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">xi</span> <span class="o">*</span> <span class="n">a</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">xi</span><span class="p">)</span> <span class="o">*</span> <span class="n">b</span>
</code></pre></div></div>

<p>with this in mind, we can rewrite our example as:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">theta</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)).</span><span class="nf">log</span><span class="p">()</span>
        <span class="n">xi</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">next_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">xi</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">next_x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.4</span><span class="p">)</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x13ec7bbb0&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_8_1.png" alt="png" /></p>

<p>Now, remember, because of the <code class="language-plaintext highlighter-rouge">hard=True</code> the forward simulation is identical to the previous case. That is, the continuous relaxation that we model with Gumbel-Softmax only affects the backward gradient propagation. Let us now recompute the jacobian:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">theta</span>
<span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x13f103160&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_10_1.png" alt="png" /></p>

<p>and now we have a gradient! The temperature parameter of the GS distribution entails a bias-variance tradeoff as explained in the previous notebook. Let’s analyze the effect here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">taus</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">n_gradient_samples</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">gradients_per_tau</span> <span class="o">=</span> <span class="p">{</span><span class="n">tau</span><span class="p">:</span> <span class="p">[]</span> <span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">taus</span><span class="p">}</span>
<span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">taus</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_gradient_samples</span><span class="p">):</span>
        <span class="n">dx_dtheta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nf">random_walk</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="o">=</span><span class="n">n_timesteps</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">),</span>
            <span class="n">theta</span><span class="p">,</span>
            <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">dx_dtheta</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tau</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">gradients_per_tau</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">grad</span> <span class="ow">in</span> <span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">]:</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span>
        <span class="nf">sum</span><span class="p">(</span><span class="n">gradients_per_tau</span><span class="p">[</span><span class="n">tau</span><span class="p">])</span> <span class="o">/</span> <span class="n">n_gradient_samples</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="sh">"</span><span class="s">$\tau$ = </span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s"> [mean]</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">lw</span><span class="o">=</span><span class="mi">3</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">([],</span> <span class="p">[],</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="sh">"</span><span class="s">$\tau$ = </span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span>
    <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">),</span>
    <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)),</span>
    <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">true gradient</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x13eb44b80&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_13_1.png" alt="png" /></p>

<p>First let’s address how we obtained the true gradient value by the black dashed line $y=x$. Since the random walk is a linear model, we can write</p>

\[\begin{align}
\frac{\partial}{\partial \theta} \mathbb E[x_N]  &amp; = \frac{\partial}{\partial \theta} \mathbb E\left[ \sum_{j=1}^{N} 2 \mathrm{Bernoulli(\theta)} - 1 \right]\\
                                                 &amp; = \frac{\partial}{\partial \theta} (2  N  \theta - N) \\
                                                 &amp; = 2 N
\end{align}\]

<p>Second, we observe that for decreasing values of $\tau$, the mean of the gradient estimate gets closer to the true values, but the variance increases significantly. In this particular case, we may do better with $\tau=0.5$ than $\tau=0.1$ since the small bias is an acceptable trade for a big reduction in variance.</p>

<h1 id="2-sir-model">2. SIR model</h1>

<p>Let us know code a differentiable Susceptible-Infected-Recovered epidemiological model. The ABM is a discretization of the system of equations</p>

\[\begin{align}
\frac{\mathrm{d} S}{\mathrm{d} t} &amp;= - \beta SI \\
\frac{\mathrm{d} I}{\mathrm{d} t} &amp;=  \beta SI - \gamma I\\
\frac{\mathrm{d} R}{\mathrm{d} t} &amp;= \gamma I \\
\end{align}\]

<p>where $\beta$ is the effective contact rate (higher values correspond to faster disease spread), and $\gamma$ is the recovery rate (e.g., a $\gamma =0.05$ corresponds to a mean recovery time of 20 days), and S, I, R are the fraction of susceptible, infected, and recovered individuals respectively.</p>

<p>The corresponding ABM can be obtained by considering a collection of $N$ agents. At each time-step, the probability of agent $i$ getting infected is given by</p>

\[p_i = 1 - \exp{\left(-\beta I \Delta t\right)},\]

<p>where $I$ is the fraction of individuals infected at this time, and $\Delta t$ is the duration of the time-step. Likewise, an infected individual can recover with probability</p>

\[q_i = 1 - \exp{\left(-\gamma \Delta t\right)}.\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">p</span><span class="p">]).</span><span class="nf">t</span><span class="p">().</span><span class="nf">log</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:,</span> <span class="mi">0</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">sir</span><span class="p">(</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">):</span>
    <span class="c1"># here S, I, R denote arrays of size (n_agents, ) with 1 or 0 depending on their state.
</span>    <span class="n">I</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">initial_fraction_infected</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">n_agents</span><span class="p">),</span> <span class="n">tau</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">I</span>
    <span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n_agents</span><span class="p">)</span>
    <span class="n">infections_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([])</span>
    <span class="n">recoveries_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">):</span>
        <span class="c1"># sample probs
</span>        <span class="n">probs_infected</span> <span class="o">=</span> <span class="n">S</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">beta</span> <span class="o">*</span> <span class="nf">sum</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_agents</span> <span class="o">*</span> <span class="n">delta_t</span><span class="p">))</span>
        <span class="n">probs_infected</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">probs_infected</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">is_infected</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">probs_infected</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
        <span class="n">probs_recovery</span> <span class="o">=</span> <span class="n">I</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">delta_t</span><span class="p">))</span>
        <span class="n">probs_recovery</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">probs_recovery</span><span class="p">,</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">is_recovered</span> <span class="o">=</span> <span class="nf">sample_bernoulli</span><span class="p">(</span><span class="n">probs_recovery</span><span class="p">,</span> <span class="n">tau</span><span class="p">)</span>
        <span class="c1"># update
</span>        <span class="n">S</span> <span class="o">=</span> <span class="n">S</span> <span class="o">-</span> <span class="n">is_infected</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">I</span> <span class="o">+</span> <span class="n">is_infected</span> <span class="o">-</span> <span class="n">is_recovered</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">is_recovered</span>
        <span class="c1"># save
</span>        <span class="n">infections_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span>
            <span class="p">(</span><span class="n">infections_per_timestep</span><span class="p">,</span> <span class="n">is_infected</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_agents</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">recoveries_per_timestep</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span>
            <span class="p">(</span><span class="n">recoveries_per_timestep</span><span class="p">,</span> <span class="n">is_recovered</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">n_agents</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">infections_per_timestep</span><span class="p">,</span> <span class="n">recoveries_per_timestep</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">initial_fraction_infected</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_agents</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">60</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">inf_t</span><span class="p">,</span> <span class="n">rec_t</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">inf_t</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">rec_t</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time [ day ]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Population fraction</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x13f188f70&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_18_1.png" alt="png" /></p>

<h2 id="22-gradients-of-the-sir-model">2.2 Gradients of the SIR model</h2>

<p>Similarly as the random walk model, we can now easily obtain the gradients of the infections and recoveries time-series.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fixed parameters
</span><span class="n">initial_fraction_infected</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">n_agents</span> <span class="o">=</span> <span class="mi">1_000</span>
<span class="n">delta_t</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">n_timesteps</span> <span class="o">=</span> <span class="mi">60</span>
<span class="k">def</span> <span class="nf">faux</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">x</span>
    <span class="k">return</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>

<span class="n">beta</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">jacobian</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">jacobian</span><span class="p">(</span><span class="n">faux</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">]),</span> <span class="n">vectorize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">Gradients resp. to $\beta$</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">jacobian</span><span class="p">[</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">Gradients resp. to $\gamma $</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Time [ day ]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Gradient</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Text(0, 0.5, 'Gradient')
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_21_1.png" alt="png" /></p>

<h2 id="23-fitting-by-gradient-descent">2.3 Fitting by gradient descent</h2>

<p>It’s about time to do what we set up to do with this tutorial: using the gradients to calibrate our model.</p>

<p>Let’s assume we have an observed (multivariate) time-series $\mathbf y$. We want to compute the “optimal” values of $\beta$ and $\gamma$ that generate $\mathbf x$ that is as close as possible as $\mathbf y$. More specifically, we want to compute</p>

\[(\beta^*, \gamma^*) = \argmin_{(\beta, \gamma)} \ell \, \left(\mathbf x(\beta, \gamma), \mathbf y\right)\]

<p>where $\ell$ is an appropritate distance function. In this case, we will just consider the standard L2 loss.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's generate some fake observation data first
</span><span class="n">true_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">true_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">true_inf</span><span class="p">,</span> <span class="n">true_rec</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span>
    <span class="n">true_beta</span><span class="p">,</span> <span class="n">true_gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span>
<span class="p">)</span>
<span class="c1"># let's add some observation noise
</span><span class="n">sigma_obs</span> <span class="o">=</span> <span class="mf">0.05</span>
<span class="n">true_inf</span> <span class="o">=</span> <span class="n">true_inf</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma_obs</span> <span class="o">*</span> <span class="n">true_inf</span>
<span class="n">true_rec</span> <span class="o">=</span> <span class="n">true_rec</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">n_timesteps</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma_obs</span> <span class="o">*</span> <span class="n">true_rec</span>

<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_inf</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily infections</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">true_rec</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">daily recoveries</span><span class="sh">"</span><span class="p">)</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[&lt;matplotlib.lines.Line2D at 0x14f4fd120&gt;]
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_23_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># now let's find the best fit through gradient descent and try to recover the true parameters
# one thing to consider is that gradient descent is not a constrained optimization method
# so to avoid beta and gamma to go negative we will use a log transformation
</span><span class="n">log_beta</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">log_gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">,</span> <span class="n">obs_inf</span><span class="p">,</span> <span class="n">obs_rec</span><span class="p">):</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">log_beta</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">**</span> <span class="n">log_gamma</span>
    <span class="n">inf</span><span class="p">,</span> <span class="n">rec</span> <span class="o">=</span> <span class="nf">sir</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">initial_fraction_infected</span><span class="p">,</span> <span class="n">n_agents</span><span class="p">,</span> <span class="n">n_timesteps</span><span class="p">,</span> <span class="n">delta_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">inf</span> <span class="o">-</span> <span class="n">obs_inf</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">rec</span> <span class="o">-</span> <span class="n">obs_rec</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">loss_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">beta_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">gamma_hist</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">optimizer</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">([</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">)):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
    <span class="n">l</span> <span class="o">=</span> <span class="nf">loss</span><span class="p">(</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">,</span> <span class="n">true_inf</span><span class="p">,</span> <span class="n">true_rec</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
    <span class="c1"># clip norm this is important to avoid exploding gradients
</span>    <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">([</span><span class="n">log_beta</span><span class="p">,</span> <span class="n">log_gamma</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
    <span class="n">loss_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">l</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    <span class="n">beta_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_beta</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
    <span class="n">gamma_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="n">log_gamma</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100%|██████████| 200/200 [01:22&lt;00:00,  2.43it/s]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's see the results
</span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">loss_hist</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Estimated parameters</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">beta_hist</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Estimated beta</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">C0</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">(</span><span class="n">gamma_hist</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Estimated gamma</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">C1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axhline</span><span class="p">(</span><span class="n">true_beta</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">C0</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">True beta</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axhline</span><span class="p">(</span><span class="n">true_gamma</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">C1</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">True gamma</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">()</span>

</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;matplotlib.legend.Legend at 0x14f4d9300&gt;
</code></pre></div></div>

<p><img src="03-differentiable-abm_files/03-differentiable-abm_25_1.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
</code></pre></div></div>

      </div>
    </main>
  </body>
</html>